{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images in the folder is: 2000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_folder(folder_path):\n",
    "    # List of valid image extensions\n",
    "    valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff', '.webp')\n",
    "\n",
    "    # Get the list of files in the directory\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Count the number of images\n",
    "    image_count = sum(1 for file in files if file.lower().endswith(valid_extensions))\n",
    "    \n",
    "    return image_count\n",
    "\n",
    "# Path to your image directory\n",
    "image_dir = './static/image'\n",
    "\n",
    "# Get the number of images\n",
    "num_images = count_images_in_folder(image_dir)\n",
    "\n",
    "print(f\"The number of images in the folder is: {num_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [1:56:18<00:00, 27.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to captions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "image_dir = './static/image'\n",
    "batch_size = 8\n",
    "data = []\n",
    "\n",
    "# Function to process a batch of images\n",
    "def process_batch(image_paths):\n",
    "    images = [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    out = model.generate(**inputs, max_new_tokens=20)\n",
    "    captions = processor.batch_decode(out, skip_special_tokens=True)\n",
    "    return captions\n",
    "\n",
    "# Get list of all image paths\n",
    "image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Process images in batches\n",
    "for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    captions = process_batch(batch_paths)\n",
    "    for img_path, caption in zip(batch_paths, captions):\n",
    "        image_id = os.path.basename(img_path)\n",
    "        data.append({'image_id': image_id, 'caption': caption})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "csv_path = 'captions.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Captions saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vaishnavishinde/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows :  51774\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic Depth Estimation</td>\n",
       "      <td>Stereo matching is one of the widely used techniques for inferring depth from\\nstereo images owing to its robustness and speed. It has become one of the major\\ntopics of research since it finds its applications in autonomous driving,\\nrobotic navigation, 3D reconstruction, and many other fields. Finding pixel\\ncorrespondences in non-textured, occluded and reflective areas is the major\\nchallenge in stereo matching. Recent developments have shown that semantic cues\\nfrom image segmentation can be used to improve the results of stereo matching.\\nMany deep neural network architectures have been proposed to leverage the\\nadvantages of semantic segmentation in stereo matching. This paper aims to give\\na comparison among the state of art networks both in terms of accuracy and in\\nterms of speed which are of higher importance in real-time applications.</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging</td>\n",
       "      <td>The recent advancements in artificial intelligence (AI) combined with the\\nextensive amount of data generated by today's clinical systems, has led to the\\ndevelopment of imaging AI solutions across the whole value chain of medical\\nimaging, including image reconstruction, medical image segmentation,\\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\\nfuture potential of AI in medical imaging, many stakeholders are concerned of\\nthe potential risks and ethical implications of imaging AI solutions, which are\\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\\nin critical clinical applications. Despite these concerns and risks, there are\\ncurrently no concrete guidelines and best practices for guiding future AI\\ndevelopments in medical imaging towards increased trust, safety and adoption.\\nTo bridge this gap, this paper introduces a careful selection of guiding\\nprinciples drawn from the accumulated experiences, consensus, and best\\npractices from five large European projects on AI in Health Imaging. These\\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\\nand (vi) Explainability. In a step-by-step approach, these guidelines are\\nfurther translated into a framework of concrete recommendations for specifying,\\ndeveloping, evaluating, and deploying technically, clinically and ethically\\ntrustworthy AI solutions into clinical practice.</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions for Semi-supervised Medical Image Segmentation</td>\n",
       "      <td>In this paper, we proposed a novel mutual consistency network (MC-Net+) to\\neffectively exploit the unlabeled hard regions for semi-supervised medical\\nimage segmentation. The MC-Net+ model is motivated by the observation that deep\\nmodels trained with limited annotations are prone to output highly uncertain\\nand easily mis-classified predictions in the ambiguous regions (e.g. adhesive\\nedges or thin branches) for the image segmentation task. Leveraging these\\nregion-level challenging samples can make the semi-supervised segmentation\\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\\nof two new designs. First, the model contains one shared encoder and multiple\\nsightly different decoders (i.e. using different up-sampling strategies). The\\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, a new\\nmutual consistency constraint is enforced between one decoder's probability\\noutput and other decoders' soft pseudo labels. In this way, we minimize the\\nmodel's uncertainty during training and force the model to generate invariant\\nand low-entropy results in such challenging areas of unlabeled data, in order\\nto learn a generalized feature representation. We compared the segmentation\\nresults of the MC-Net+ with five state-of-the-art semi-supervised approaches on\\nthree public medical datasets. Extension experiments with two common\\nsemi-supervised settings demonstrate the superior performance of our model over\\nother existing methods, which sets a new state of the art for semi-supervised\\nmedical image segmentation.</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervised 3D Left Atrium Segmentation</td>\n",
       "      <td>Consistency training has proven to be an advanced semi-supervised framework\\nand achieved promising results in medical image segmentation tasks through\\nenforcing an invariance of the predictions over different views of the inputs.\\nHowever, with the iterative updating of model parameters, the models would tend\\nto reach a coupled state and eventually lose the ability to exploit unlabeled\\ndata. To address the issue, we present a novel semi-supervised segmentation\\nmodel based on parameter decoupling strategy to encourage consistent\\npredictions from diverse views. Specifically, we first adopt a two-branch\\nnetwork to simultaneously produce predictions for each image. During the\\ntraining process, we decouple the two prediction branch parameters by quadratic\\ncosine distance to construct different views in latent space. Based on this,\\nthe feature extractor is constrained to encourage the consistency of\\nprobability maps generated by classifiers under diversified features. In the\\noverall training process, the parameters of feature extractor and classifiers\\nare updated alternately by consistency regularization operation and decoupling\\noperation to gradually improve the generalization performance of the model. Our\\nmethod has achieved a competitive result over the state-of-the-art\\nsemi-supervised methods on the Atrial Segmentation Challenge dataset,\\ndemonstrating the effectiveness of our framework. Code is available at\\nhttps://github.com/BX0903/PDC.</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interior Sensing in Automotive Industry</td>\n",
       "      <td>To ensure safety in automated driving, the correct perception of the\\nsituation inside the car is as important as its environment. Thus, seat\\noccupancy detection and classification of detected instances play an important\\nrole in interior sensing. By the knowledge of the seat occupancy status, it is\\npossible to, e.g., automate the airbag deployment control. Furthermore, the\\npresence of a driver, which is necessary for partially automated driving cars\\nat the automation levels two to four can be verified. In this work, we compare\\ndifferent statistical methods from the field of image segmentation to approach\\nthe problem of background-foreground segmentation in camera based interior\\nsensing. In the recent years, several methods based on different techniques\\nhave been developed and applied to images or videos from different\\napplications. The peculiarity of the given scenarios of interior sensing is,\\nthat the foreground instances and the background both contain static as well as\\ndynamic elements. In data considered in this work, even the camera position is\\nnot completely fixed. We review and benchmark three different methods ranging,\\ni.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neural\\nnetwork, namely a Mask R-CNN. In particular, the limitations of the classical\\nmethods, GMM and Morphological Snakes, for interior sensing are shown.\\nFurthermore, it turns, that it is possible to overcome these limitations by\\ndeep learning, e.g.\\ using a Mask R-CNN. Although only a small amount of ground\\ntruth data was available for training, we enabled the Mask R-CNN to produce\\nhigh quality background-foreground masks via transfer learning. Moreover, we\\ndemonstrate that certain augmentation as well as pre- and post-processing\\nmethods further enhance the performance of the investigated methods.</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re # for regular expressions\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "import string\n",
    "from IPython.display import display, HTML\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "data = pd.read_csv(\"static/arxiv_data.csv\", header='infer')\n",
    "print(\"Total rows : \", len(data))\n",
    "display(HTML(data.head().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    # Text normalization\n",
    "    expanded_words = []    \n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shortened words\n",
    "      expanded_words.append(contractions.fix(word))  \n",
    "   \n",
    "    text = ' '.join(expanded_words)\n",
    "    # Remove punctuations\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  \n",
    "    tokens = text.lower().split()\n",
    "    # Stemming and stop-word removal\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of inverted index  71580\n"
     ]
    }
   ],
   "source": [
    "# Inverted index construction\n",
    "def construct_inverted_index(df):\n",
    "  dictionary = {} # inverted index\n",
    " \n",
    "  for index, row in df.iterrows():\n",
    "    try:\n",
    "      abstract_tokens = preprocess(row.summaries) # convert paper abstracts to tokens\n",
    "      for token in abstract_tokens:\n",
    "        if token not in dictionary: # add the document index to the postings of each term in the tokenized text\n",
    "          dictionary[token] = [index]\n",
    "        else:\n",
    "          dictionary[token].append(index)\n",
    "    except:\n",
    "      continue\n",
    " \n",
    "  #removing duplicates\n",
    "  dictionary = {a:set(b) for a, b in dictionary.items()}\n",
    " \n",
    "  return dictionary\n",
    "inverted_index = construct_inverted_index(data)\n",
    "print(\"Size of inverted index \", len(inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_infix_expression(expression):\n",
    "    return expression.split()\n",
    "def infix_to_postfix(tokens):\n",
    "    precedence = {\"AND\": 2, \"NOT\": 3, \"OR\":1 } # set the precedence of operators for postfix expression\n",
    "    stack = []\n",
    "    postfix = []\n",
    "    for token in tokens:\n",
    "        if token in precedence: # add operands first and then operators\n",
    "            while stack and precedence.get(stack[-1], 0) >= precedence[token]:\n",
    "                postfix.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        elif token == '(':\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            while stack and stack[-1] != '(':\n",
    "                postfix.append(stack.pop())\n",
    "            stack.pop()\n",
    "        else:\n",
    "            postfix.append(token)\n",
    "    while stack:\n",
    "        postfix.append(stack.pop())\n",
    "    return postfix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_postfix(postfix):\n",
    "    stemmer = PorterStemmer()\n",
    "    stack = []\n",
    "    for token in postfix:\n",
    "        if token == \"AND\": # take intersection of postings\n",
    "            set2 = stack.pop()\n",
    "            set1 = stack.pop()\n",
    "            result = set1.intersection(set2)\n",
    "            stack.append(result)\n",
    "        elif token == \"NOT\": # finding all documents that are not in the postings list\n",
    "            set1 = stack.pop()\n",
    "            set2 = set(range(len(data)))\n",
    "            result = set2.difference(set1)\n",
    "            stack.append(result)\n",
    "        elif token == \"OR\": # take union of postings\n",
    "            set1 = stack.pop()\n",
    "            set2 = stack.pop()\n",
    "            stack.append(set1.union(set2))  # Convert token to a set\n",
    "        else: # retrive the posting of the stemmed token\n",
    "          stack.append(inverted_index.get(stemmer.stem(token), set()))\n",
    "       \n",
    "    return stack[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cancer', 'detection', 'AND', 'machine', 'AND', 'learning', 'AND', 'lung', 'NOT', 'AND']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Rigorous Machine Learning Analysis Pipeline for Biomedical Binary Classification: Application in Pancreatic Cancer Nested Case-control Studies with Implications for Bias Assessments</td>\n",
       "      <td>Machine learning (ML) offers a collection of powerful approaches for detecting and modeling associations, often applied to data having a large number of features and/or complex associations. Currently, there are many tools to facilitate implementing custom ML analyses (e.g. scikit-learn). Interest is also increasing in automated ML packages, which can make it easier for non-experts to apply ML and have the potential to improve model performance. ML permeates most subfields of biomedical research with varying levels of rigor and correct usage. Tremendous opportunities offered by ML are frequently offset by the challenge of assembling comprehensive analysis pipelines, and the ease of ML misuse. In this work we have laid out and assembled a complete, rigorous ML analysis pipeline focused on binary classification (i.e. case/control prediction), and applied this pipeline to both simulated and real world data. At a high level, this 'automated' but customizable pipeline includes a) exploratory analysis, b) data cleaning and transformation, c) feature selection, d) model training with 9 established ML algorithms, each with hyperparameter optimization, and e) thorough evaluation, including appropriate metrics, statistical analyses, and novel visualizations. This pipeline organizes the many subtle complexities of ML pipeline assembly to illustrate best practices to avoid bias and ensure reproducibility. Additionally, this pipeline is the first to compare established ML algorithms to 'ExSTraCS', a rule-based ML algorithm with the unique capability of interpretably modeling heterogeneous patterns of association. While designed to be widely applicable we apply this pipeline to an epidemiological investigation of established and newly identified risk factors for pancreatic cancer to evaluate how different sources of bias might be handled by ML algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transport-based analysis, modeling, and learning from signal and data distributions</td>\n",
       "      <td>Transport-based techniques for signal and data analysis have received increased attention recently. Given their abilities to provide accurate generative models for signal intensities and other data distributions, they have been used in a variety of applications including content-based retrieval, cancer detection, image super-resolution, and statistical machine learning, to name a few, and shown to produce state of the art in several applications. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of data distributions. Here we provide an overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>labelCloud: A Lightweight Domain-Independent Labeling Tool for 3D Object Detection in Point Clouds</td>\n",
       "      <td>Within the past decade, the rise of applications based on artificial intelligence (AI) in general and machine learning (ML) in specific has led to many significant contributions within different domains. The applications range from robotics over medical diagnoses up to autonomous driving. However, nearly all applications rely on trained data. In case this data consists of 3D images, it is of utmost importance that the labeling is as accurate as possible to ensure high-quality outcomes of the ML models. Labeling in the 3D space is mostly manual work performed by expert workers, where they draw 3D bounding boxes around target objects the ML model should later automatically identify, e.g., pedestrians for autonomous driving or cancer cells within radiography.   While a small range of recent 3D labeling tools exist, they all share three major shortcomings: (i) they are specified for autonomous driving applications, (ii) they lack convenience and comfort functions, and (iii) they have high dependencies and little flexibility in data format. Therefore, we propose a novel labeling tool for 3D object detection in point clouds to address these shortcomings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>labelCloud: A Lightweight Domain-Independent Labeling Tool for 3D Object Detection in Point Clouds</td>\n",
       "      <td>Within the past decade, the rise of applications based on artificial intelligence (AI) in general and machine learning (ML) in specific has led to many significant contributions within different domains. The applications range from robotics over medical diagnoses up to autonomous driving. However, nearly all applications rely on trained data. In case this data consists of 3D images, it is of utmost importance that the labeling is as accurate as possible to ensure high-quality outcomes of the ML models. Labeling in the 3D space is mostly manual work performed by expert workers, where they draw 3D bounding boxes around target objects the ML model should later automatically identify, e.g., pedestrians for autonomous driving or cancer cells within radiography.   While a small range of recent 3D labeling tools exist, they all share three major shortcomings: (i) they are specified for autonomous driving applications, (ii) they lack convenience and comfort functions, and (iii) they have high dependencies and little flexibility in data format. Therefore, we propose a novel labeling tool for 3D object detection in point clouds to address these shortcomings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overhead MNIST: A Benchmark Satellite Dataset</td>\n",
       "      <td>The research presents an overhead view of 10 important objects and follows the general formatting requirements of the most popular machine learning task: digit recognition with MNIST. This dataset offers a public benchmark extracted from over a million human-labelled and curated examples. The work outlines the key multi-class object identification task while matching with prior work in handwriting, cancer detection, and retail datasets. A prototype deep learning approach with transfer learning and convolutional neural networks (MobileNetV2) correctly identifies the ten overhead classes with an average accuracy of 96.7%. This model exceeds the peak human performance of 93.9%. For upgrading satellite imagery and object recognition, this new dataset benefits diverse endeavors such as disaster relief, land use management, and other traditional remote sensing tasks. The work extends satellite benchmarks with new capabilities to identify efficient and compact algorithms that might work on-board small satellites, a practical task for future multi-sensor constellations. The dataset is available on Kaggle and Github.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Automatic Mass Detection in Breast Using Deep Convolutional Neural Network and SVM Classifier</td>\n",
       "      <td>Mammography is the most widely used gold standard for screening breast cancer, where, mass detection is considered as the prominent step. Detecting mass in the breast is, however, an arduous problem as they usually have large variations between them in terms of shape, size, boundary, and texture. In this literature, the process of mass detection is automated with the use of transfer learning techniques of Deep Convolutional Neural Networks (DCNN). Pre-trained VGG19 network is used to extract features which are then followed by bagged decision tree for features selection and then a Support Vector Machine (SVM) classifier is trained and used for classifying between the mass and non-mass. Area Under ROC Curve (AUC) is chosen as the performance metric, which is then maximized during classifier selection and hyper-parameter tuning. The robustness of the two selected type of classifiers, C-SVM, and \\u{psion}-SVM, are investigated with extensive experiments before selecting the best performing classifier. All experiments in this paper were conducted using the INbreast dataset. The best AUC obtained from the experimental results is 0.994 +/- 0.003 i.e. [0.991, 0.997]. Our results conclude that by using pre-trained VGG19 network, high-level distinctive features can be extracted from Mammograms which when used with the proposed SVM classifier is able to robustly distinguish between the mass and non-mass present in breast.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Improving Prostate Cancer Detection with Breast Histopathology Images</td>\n",
       "      <td>Deep neural networks have introduced significant advancements in the field of machine learning-based analysis of digital pathology images including prostate tissue images. With the help of transfer learning, classification and segmentation performance of neural network models have been further increased. However, due to the absence of large, extensively annotated, publicly available prostate histopathology datasets, several previous studies employ datasets from well-studied computer vision tasks such as ImageNet dataset. In this work, we propose a transfer learning scheme from breast histopathology images to improve prostate cancer detection performance. We validate our approach on annotated prostate whole slide images by using a publicly available breast histopathology dataset as pre-training. We show that the proposed cross-cancer approach outperforms transfer learning from ImageNet dataset.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deep learning analysis of breast MRIs for prediction of occult invasive disease in ductal carcinoma in situ</td>\n",
       "      <td>Purpose: To determine whether deep learning-based algorithms applied to breast MR images can aid in the prediction of occult invasive disease following the di- agnosis of ductal carcinoma in situ (DCIS) by core needle biopsy. Material and Methods: In this institutional review board-approved study, we analyzed dynamic contrast-enhanced fat-saturated T1-weighted MRI sequences of 131 patients at our institution with a core needle biopsy-confirmed diagnosis of DCIS. The patients had no preoperative therapy before breast MRI and no prior history of breast cancer. We explored two different deep learning approaches to predict whether there was a hidden (occult) invasive component in the analyzed tumors that was ultimately detected at surgical excision. In the first approach, we adopted the transfer learning strategy, in which a network pre-trained on a large dataset of natural images is fine-tuned with our DCIS images. Specifically, we used the GoogleNet model pre-trained on the ImageNet dataset. In the second approach, we used a pre-trained network to extract deep features, and a support vector machine (SVM) that utilizes these features to predict the upstaging of the DCIS. We used 10-fold cross validation and the area under the ROC curve (AUC) to estimate the performance of the predictive models. Results: The best classification performance was obtained using the deep features approach with GoogleNet model pre-trained on ImageNet as the feature extractor and a polynomial kernel SVM used as the classifier (AUC = 0.70, 95% CI: 0.58- 0.79). For the transfer learning based approach, the highest AUC obtained was 0.53 (95% CI: 0.41-0.62). Conclusion: Convolutional neural networks could potentially be used to identify occult invasive disease in patients diagnosed with DCIS at the initial core needle biopsy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Automatic Mass Detection in Breast Using Deep Convolutional Neural Network and SVM Classifier</td>\n",
       "      <td>Mammography is the most widely used gold standard for screening breast cancer, where, mass detection is considered as the prominent step. Detecting mass in the breast is, however, an arduous problem as they usually have large variations between them in terms of shape, size, boundary, and texture. In this literature, the process of mass detection is automated with the use of transfer learning techniques of Deep Convolutional Neural Networks (DCNN). Pre-trained VGG19 network is used to extract features which are then followed by bagged decision tree for features selection and then a Support Vector Machine (SVM) classifier is trained and used for classifying between the mass and non-mass. Area Under ROC Curve (AUC) is chosen as the performance metric, which is then maximized during classifier selection and hyper-parameter tuning. The robustness of the two selected type of classifiers, C-SVM, and \\u{psion}-SVM, are investigated with extensive experiments before selecting the best performing classifier. All experiments in this paper were conducted using the INbreast dataset. The best AUC obtained from the experimental results is 0.994 +/- 0.003 i.e. [0.991, 0.997]. Our results conclude that by using pre-trained VGG19 network, high-level distinctive features can be extracted from Mammograms which when used with the proposed SVM classifier is able to robustly distinguish between the mass and non-mass present in breast.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Transfer Learning with Ensembles of Deep Neural Networks for Skin Cancer Detection in Imbalanced Data Sets</td>\n",
       "      <td>Several machine learning techniques for accurate detection of skin cancer from medical images have been reported. Many of these techniques are based on pre-trained convolutional neural networks (CNNs), which enable training the models based on limited amounts of training data. However, the classification accuracy of these models still tends to be severely limited by the scarcity of representative images from malignant tumours. We propose a novel ensemble-based CNN architecture where multiple CNN models, some of which are pre-trained and some are trained only on the data at hand, along with auxiliary data in the form of metadata associated with the input images, are combined using a meta-learner. The proposed approach improves the model's ability to handle limited and imbalanced data. We demonstrate the benefits of the proposed technique using a dataset with 33126 dermoscopic images from 2056 patients. We evaluate the performance of the proposed technique in terms of the F1-measure, area under the ROC curve (AUC-ROC), and area under the PR-curve (AUC-PR), and compare it with that of seven different benchmark methods, including two recent CNN-based techniques. The proposed technique compares favourably in terms of all the evaluation metrics.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "infix_expression = \"cancer AND detection AND machine AND learning AND NOT lung\"\n",
    "# Tokenize the expression\n",
    "tokens = tokenize_infix_expression(infix_expression)\n",
    "# Convert to postfix notation\n",
    "postfix_expression = infix_to_postfix(tokens)\n",
    "print(postfix_expression)\n",
    "# # Evaluate the postfix expression\n",
    "result = evaluate_postfix(postfix_expression)\n",
    "titles = []\n",
    "abstracts = []\n",
    "for res in list(result)[:10]:\n",
    "  titles.append(data.iloc[res].titles)\n",
    "  abstracts.append(data.iloc[res].summaries.replace(\"\\n\", \" \"))\n",
    "results = pd.DataFrame({\"Title\": titles, \"Abstract\": abstracts} )\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
