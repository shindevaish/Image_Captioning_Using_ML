import os
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import pandas as pd
import torch
from tqdm import tqdm

processor=BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model=BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

device =torch.device("cpu")
model.to(device)

folder='/Users/vaishnavishinde/Desktop/Image_Captioning_ML_Clean/static/images_small'
batch_size=8
data=[]

def process_batch(paths):
    image=[Image.open(path).convert('RGB') for path in paths]
    inputs=processor(images=image,return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    out=model.generate(**inputs,max_new_tokens=20)
    caption=processor.decode(out[0],skip_special_tokens=True)
    return caption

paths = [os.path.join(folder, img) for img in os.listdir(folder) if img.endswith(('.png', '.jpg', '.jpeg'))]


for i in tqdm(range(0,len(paths),batch_size)):
    batch_path=paths[i:i+batch_size]
    captions=process_batch(batch_path)
    for path,caption in zip(batch_path,captions):
        image_id=os.path.basename(path)
        data.append({'image_id': image_id, 'caption' : caption})

# for name in os.listdir(folder):
#     path=os.path.join(folder,name)
#     image=Image.open(path).convert('RGB')

#     inputs=processor(images=image,return_tensors="pt")
#     out=model.generate(**inputs)
#     caption=processor.decode(out[0],skip_special_tokens=True)

#     data.append({'image_id' : name , 'caption' : caption})

df=pd.DataFrame(data)

csv_path='image_caption.csv'
df.to_csv(csv_path,index=False)

print("Caption saved to {csv_path}")
 